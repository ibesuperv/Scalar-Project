# **AI Python Code Fixer â€” Offline Debugging Environment**

A fully offline **AI-powered Python debugging workspace** consisting of:

* ğŸ§  **Backend (Python + Flask + Ollama)** using
  **Gemma2:2b-instruct-q4_K_M** for fixing code
* ğŸ’» **Frontend (React + TypeScript + Vite)** with
  live Python execution using **Pyodide** (WebAssembly Python)

This project provides a split-screen interface where the user can:

* Write Python code
* Execute it locally (via Pyodide)
* Send broken code to the backend
* Get **AI-repaired Python code**
* View formatted, highlighted output in a comparison panel

Everything works **100% offline** after installation â€”
No internet required.

---

# ğŸš€ **Features**

### ğŸ”§ **AI Code Fixer (Backend)**

* Sends user-written Python code to **Ollama LLM**
* Model attempts to fix syntax errors, logic errors & crashes
* Annotates each fix with `# FIXED: ...` comments
* Formats final code using **Black**
* Returns corrected code back to frontend

### ğŸ§ª **Local Python Execution (Frontend)**

Uses **Pyodide**:

* Runs Python directly in the browser
* No server needed for executing code
* Captures stdout/stderr
* Displays colored logs

### ğŸ–¥ï¸ **UI Features**

* Code editor with syntax highlight (Monaco Editor)
* Live right-panel showing AI-fixed Python code
* Terminal showing logs/output
* Toolbar with: 

  * Run button
  * Fix button (with animated timer + overlay â€œFixingâ€¦â€ popup)
* Clean Tailwind UI

---


# ğŸ“‚ **Folder Structure**

## **Frontend/**

```
frontend/
â”‚
â”œâ”€ utils/
â”‚   â””â”€ pyodideManager.ts
â”‚
â”œâ”€ components/
â”‚   â”œâ”€ Button.tsx
â”‚   â”œâ”€ Editor.tsx
â”‚   â”œâ”€ Header.tsx
â”‚   â”œâ”€ OutputTerminal.tsx
â”‚   â”œâ”€ RightPanel.tsx
â”‚   â”œâ”€ Terminal.tsx
â”‚   â””â”€ Toolbar.tsx
|
â”‚â”€ App.tsx
â”‚â”€ index.tsx
â”‚â”€ index.html
â”‚â”€ types.tsx
â”‚â”€ package.json
â”‚â”€ tsconfig.json
â”‚â”€ vite.config.ts

```

---

## **Backend/**

```
backend/
â”‚â”€ patcher.py        â† Main Flask + Ollama API
â”‚â”€ requirements.txt
```

---


# ğŸ§  **LLM Used**

### **Gemma2 (2B) â€” `gemma2:2b-instruct-q4_K_M`**

Runs fully offline inside **Ollama**.

Role in project:

* Reads broken Python code
* Analyzes runtime error text (generated by executing code in backend sandbox)
* Repairs the code
* Adds FIX comments for transparency
* Returns only final Python script

---

# âš™ï¸ **How the System Works (High Level Overview)**

## 1ï¸âƒ£ **User writes Python code in the frontend**

* Live editor updates state
* User can run code â†’ executed in **Pyodide** (browser)

## 2ï¸âƒ£ **User clicks â€œFixâ€**

Frontend sends code â†’ Backend:

```
POST /api/fix-code
{
  "code": "user's broken code"
}
```

A loading popup appears:

* Animated Tailwind â€œFixingâ€¦â€
* Timer counting the fix duration

## 3ï¸âƒ£ **Backend (Flask) performs the debugging**

* Executes code safely â†’ extracts errors
* Writes prompt including:

  * original code
  * extracted traceback
  * strict rules for formatting
* Sends prompt to **Ollama**
* Extracts only raw python block
* Formats with **Black**
* Returns JSON containing fixed code

## 4ï¸âƒ£ **Frontend receives AI-fixed code**

* Displays it in Right Panel with Prism highlighting
* Optionally replaces user input code
* Popup disappears

Everything is client-side or local-machine-only.

---


# ğŸ› ï¸ **Technologies Used**

### **Frontend**

* React + TypeScript
* Vite
* TailwindCSS
* Monaco Editor
* PrismJS (syntax highlighting for AI output)
* Pyodide (Python runtime in browser)

### **Backend**

* Python 3
* Flask
* Flask-CORS
* Black (code formatter)
* Traceback (runtime error capture)
* Subprocess (Ollama process)
* Ollama + Gemma2 LLM

### **Completely Offline Stack**

* Pyodide runs Python without internet
* Ollama runs LLM locally
* No external API calls
* Fully self-contained

---

# â–¶ï¸ **How to Run the Project Locally**

## **1. Clone the repo**

```
[git clone https://github.com/your/repo.git](https://github.com/ibesuperv/Scalar-Project.git)
cd Scalar-Project
```

---

# ğŸŒ **Frontend Setup**

```
cd frontend
npm install
npm run dev
```

Frontend runs at:

ğŸ‘‰ **[http://localhost:5173](http://localhost:5173)**

---

# ğŸ”§ **Backend Setup**

## Install dependencies

```
cd backend
pip install -r requirements.txt
```

# ğŸŸ¦ **Download Ollama**

### ğŸ”— Official download page:

ğŸ‘‰ **[https://ollama.com/download](https://ollama.com/download)**

Choose your OS:

| OS          | Installer             |
| ----------- | --------------------- |
| **Windows** | `.exe` installer      |
| **macOS**   | `.pkg` installer      |
| **Linux**   | Terminal installation |

# Download the Required Model**

This project uses **Gemma2 2B Instruct (quantized)**:

```
ollama pull gemma2:2b-instruct-q4_K_M
```

Model size: **~1.6GB**
Downloaded once, works offline forever.

## Start backend server

```
python patcher.py
```

Backend runs at:

ğŸ‘‰ **[http://localhost:5000](http://localhost:5000)**

---


# Demo

ğŸ¬ **Demo Video:** [Watch Demo on Google Drive](https://drive.google.com/file/d/1xY7yaXf7ihy_wjQ1kWpeZAs-CsbtbrLO/view?usp=sharing)

A fully offline **AI-powered Python debugging workspace** consisting of:


# ğŸ‰ **Project Summary**

This project is a **full offline Python debugging IDE**, combining:

* ğŸ”¥ Modern frontend UI
* ğŸ§  Local LLM-powered Python code repair
* ğŸ Browser-based Python execution

Nothing relies on the cloud â€” extremely fast, private, and portable.
